{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU detected. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"Using GPU:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved with 2940 entries.\n",
      "Train size: 2058 | Valid size: 441 | Test size: 441\n",
      "Train Dataset:\n",
      "label\n",
      "0    1029\n",
      "1    1029\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Dataset:\n",
      "label\n",
      "1    221\n",
      "0    220\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Dataset:\n",
      "label\n",
      "0    221\n",
      "1    220\n",
      "Name: count, dtype: int64\n",
      "Shuffled datasets saved successfully!\n"
     ]
    }
   ],
   "source": [
    "datasets = {\"dataset_1\": \".\\AutismDataset\"}\n",
    "\n",
    "# function to extract file paths and labels\n",
    "def create_dataframe(dataset_path):\n",
    "    data = {\"file_path\": [], \"label\": []}  # dictionary to hold file paths and labels\n",
    "\n",
    "    # handle differently based on folder type\n",
    "    for sub_dir in [\"train\", \"test\", \"valid\"]:\n",
    "        folder_path = os.path.join(dataset_path, sub_dir)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(\"WARNING: Directory not found:\", folder_path)\n",
    "            continue\n",
    "\n",
    "        if sub_dir == \"valid\":\n",
    "            # valid has labeled subdirectories\n",
    "            for label_dir, label in zip([\"Autistic\", \"Non_Autistic\"], [1, 0]):\n",
    "                label_folder_path = os.path.join(folder_path, label_dir)\n",
    "                if os.path.exists(label_folder_path):\n",
    "                    for img_file in os.listdir(label_folder_path):\n",
    "                        data[\"file_path\"].append(os.path.join(label_folder_path, img_file))\n",
    "                        data[\"label\"].append(label)\n",
    "        else:\n",
    "            # `train` and `test` have images directly so infer labels from filenames\n",
    "            for img_file in os.listdir(folder_path):\n",
    "                if \"Non_Autistic\" in img_file:\n",
    "                    label = 1\n",
    "                elif \"Autistic\" in img_file:\n",
    "                    label = 0\n",
    "                else:\n",
    "                    continue  # skip unknown files\n",
    "                data[\"file_path\"].append(os.path.join(folder_path, img_file))\n",
    "                data[\"label\"].append(label)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# create DataFrame for dataset\n",
    "merged_df = create_dataframe(datasets[\"dataset_1\"])\n",
    "\n",
    "# save the dataset as a CSV file\n",
    "merged_df.to_csv(\"merged_dataset.csv\", index = False)\n",
    "print(\"Dataset saved with\", len(merged_df), \"entries.\")\n",
    "\n",
    "# split the dataset using train_test_split\n",
    "train_df, temp_df = train_test_split(merged_df, test_size = 0.3, stratify = merged_df[\"label\"], random_state = 42)\n",
    "valid_df, test_df = train_test_split(temp_df, test_size = 0.5, stratify = temp_df[\"label\"], random_state = 42)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"| Valid size:\", len(valid_df), \"| Test size:\", len(test_df))\n",
    "\n",
    "# load the merged dataset\n",
    "merged_df = pd.read_csv(\"merged_dataset.csv\")\n",
    "\n",
    "# shuffle the data\n",
    "train_df = train_df.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "valid_df = valid_df.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "test_df = test_df.sample(frac = 1, random_state = 42).reset_index(drop = True)\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "valid_df['label'] = valid_df['label'].astype(str)\n",
    "test_df['label'] = test_df['label'].astype(str)\n",
    "\n",
    "print(\"Train Dataset:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nValidation Dataset:\")\n",
    "print(valid_df['label'].value_counts())\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# save the shuffled datasets\n",
    "train_df.to_csv(\"train.csv\", index = False)\n",
    "valid_df.to_csv(\"valid.csv\", index = False)\n",
    "test_df.to_csv(\"test.csv\", index = False)\n",
    "\n",
    "print(\"Shuffled datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2058 validated image filenames belonging to 2 classes.\n",
      "Found 441 validated image filenames belonging to 2 classes.\n",
      "Found 441 validated image filenames belonging to 2 classes.\n",
      "\n",
      "Train Generator Label Distribution:\n",
      "Counter({0: 1029, 1: 1029})\n",
      "\n",
      "Validation Generator Label Distribution:\n",
      "Counter({1: 221, 0: 220})\n",
      "\n",
      "Test Generator Label Distribution:\n",
      "Counter({0: 221, 1: 220})\n"
     ]
    }
   ],
   "source": [
    "# define ImageDataGenerators with improved augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0 / 255.0,  # normalize pixel values to [0, 1]\n",
    "    rotation_range = 5,  # rotate images by up to 5 degrees\n",
    "    width_shift_range = 0.1,  # shift images horizontally by up to 10% of the width\n",
    "    height_shift_range = 0.1,  # shift images vertically by up to 10% of the height\n",
    "    horizontal_flip = True,  # flip images horizontally\n",
    "    fill_mode = 'nearest'  # fill any missing pixels after transformations\n",
    ")\n",
    "\n",
    "valid_test_datagen = ImageDataGenerator(rescale = 1.0 / 255.0)  # no augmentation for validation & test\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(dataframe = train_df, x_col = \"file_path\", y_col = \"label\", target_size = (224, 224), batch_size = 32, class_mode = \"binary\")\n",
    "\n",
    "valid_generator = valid_test_datagen.flow_from_dataframe(dataframe = valid_df, x_col = \"file_path\", y_col = \"label\", target_size = (224, 224), batch_size = 32, class_mode = \"binary\")\n",
    "\n",
    "test_generator = valid_test_datagen.flow_from_dataframe(dataframe = test_df, x_col = \"file_path\", y_col = \"label\", target_size = (224, 224), batch_size = 32, class_mode = \"binary\", shuffle = False)\n",
    "\n",
    "# Print label distribution for each generator\n",
    "print(\"\\nTrain Generator Label Distribution:\")\n",
    "print(Counter(train_generator.labels))\n",
    "\n",
    "print(\"\\nValidation Generator Label Distribution:\")\n",
    "print(Counter(valid_generator.labels))\n",
    "\n",
    "print(\"\\nTest Generator Label Distribution:\")\n",
    "print(Counter(test_generator.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Epoch 1/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6805 - accuracy: 0.6642 \n",
      "Epoch 1: val_loss improved from inf to 0.58174, saving model to model\\densenet121_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parth Garg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 818s 12s/step - loss: 0.6805 - accuracy: 0.6642 - val_loss: 0.5817 - val_accuracy: 0.6871 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4734 - accuracy: 0.7998 \n",
      "Epoch 2: val_loss improved from 0.58174 to 0.53742, saving model to model\\densenet121_model.h5\n",
      "65/65 [==============================] - 679s 10s/step - loss: 0.4734 - accuracy: 0.7998 - val_loss: 0.5374 - val_accuracy: 0.7370 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3741 - accuracy: 0.8348 \n",
      "Epoch 3: val_loss improved from 0.53742 to 0.44736, saving model to model\\densenet121_model.h5\n",
      "65/65 [==============================] - 685s 11s/step - loss: 0.3741 - accuracy: 0.8348 - val_loss: 0.4474 - val_accuracy: 0.7914 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.8693\n",
      "Epoch 4: val_loss improved from 0.44736 to 0.44003, saving model to model\\densenet121_model.h5\n",
      "65/65 [==============================] - 671s 10s/step - loss: 0.3169 - accuracy: 0.8693 - val_loss: 0.4400 - val_accuracy: 0.8209 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.8941\n",
      "Epoch 5: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 649s 10s/step - loss: 0.2708 - accuracy: 0.8941 - val_loss: 0.4863 - val_accuracy: 0.7959 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9179\n",
      "Epoch 6: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 615s 9s/step - loss: 0.2120 - accuracy: 0.9179 - val_loss: 0.6878 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9266\n",
      "Epoch 7: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 612s 9s/step - loss: 0.1972 - accuracy: 0.9266 - val_loss: 0.5438 - val_accuracy: 0.8050 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9334\n",
      "Epoch 8: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 613s 9s/step - loss: 0.1659 - accuracy: 0.9334 - val_loss: 0.6799 - val_accuracy: 0.7914 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9451\n",
      "Epoch 9: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 610s 9s/step - loss: 0.1377 - accuracy: 0.9451 - val_loss: 0.5050 - val_accuracy: 0.8481 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1214 - accuracy: 0.9529\n",
      "Epoch 10: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 610s 9s/step - loss: 0.1214 - accuracy: 0.9529 - val_loss: 1.0150 - val_accuracy: 0.7415 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9524\n",
      "Epoch 11: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 610s 9s/step - loss: 0.1288 - accuracy: 0.9524 - val_loss: 0.6774 - val_accuracy: 0.7937 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9577\n",
      "Epoch 12: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 612s 9s/step - loss: 0.1144 - accuracy: 0.9577 - val_loss: 0.6594 - val_accuracy: 0.7937 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9665\n",
      "Epoch 13: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 610s 9s/step - loss: 0.0983 - accuracy: 0.9665 - val_loss: 0.6784 - val_accuracy: 0.8277 - lr: 1.0000e-04\n",
      "Epoch 14/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9747\n",
      "Epoch 14: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 610s 9s/step - loss: 0.0765 - accuracy: 0.9747 - val_loss: 0.6514 - val_accuracy: 0.8209 - lr: 1.0000e-04\n",
      "Epoch 15/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9713\n",
      "Epoch 15: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 610s 9s/step - loss: 0.0767 - accuracy: 0.9713 - val_loss: 0.7871 - val_accuracy: 0.7778 - lr: 1.0000e-04\n",
      "Epoch 16/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9762\n",
      "Epoch 16: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 615s 9s/step - loss: 0.0724 - accuracy: 0.9762 - val_loss: 0.8096 - val_accuracy: 0.7891 - lr: 1.0000e-04\n",
      "Epoch 17/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9762 \n",
      "Epoch 17: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 694s 11s/step - loss: 0.0650 - accuracy: 0.9762 - val_loss: 0.7226 - val_accuracy: 0.8186 - lr: 1.0000e-04\n",
      "Epoch 18/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9791\n",
      "Epoch 18: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 635s 10s/step - loss: 0.0550 - accuracy: 0.9791 - val_loss: 0.7983 - val_accuracy: 0.8050 - lr: 1.0000e-04\n",
      "Epoch 19/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9762\n",
      "Epoch 19: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 618s 10s/step - loss: 0.0576 - accuracy: 0.9762 - val_loss: 0.7495 - val_accuracy: 0.8141 - lr: 1.0000e-04\n",
      "Epoch 20/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9806\n",
      "Epoch 20: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 616s 9s/step - loss: 0.0488 - accuracy: 0.9806 - val_loss: 1.1005 - val_accuracy: 0.7732 - lr: 1.0000e-04\n",
      "Epoch 21/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9830\n",
      "Epoch 21: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 613s 9s/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.7052 - val_accuracy: 0.8413 - lr: 1.0000e-04\n",
      "Epoch 22/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9674\n",
      "Epoch 22: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 615s 9s/step - loss: 0.0839 - accuracy: 0.9674 - val_loss: 0.8404 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
      "Epoch 23/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9752\n",
      "Epoch 23: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 617s 9s/step - loss: 0.0649 - accuracy: 0.9752 - val_loss: 1.5308 - val_accuracy: 0.7438 - lr: 1.0000e-04\n",
      "Epoch 24/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9825\n",
      "Epoch 24: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 619s 10s/step - loss: 0.0468 - accuracy: 0.9825 - val_loss: 0.8108 - val_accuracy: 0.8254 - lr: 1.0000e-04\n",
      "Epoch 25/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9854\n",
      "Epoch 25: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 618s 10s/step - loss: 0.0413 - accuracy: 0.9854 - val_loss: 0.9090 - val_accuracy: 0.8390 - lr: 1.0000e-04\n",
      "Epoch 26/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9820\n",
      "Epoch 26: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 622s 10s/step - loss: 0.0497 - accuracy: 0.9820 - val_loss: 1.1478 - val_accuracy: 0.7959 - lr: 1.0000e-04\n",
      "Epoch 27/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9888\n",
      "Epoch 27: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 621s 10s/step - loss: 0.0304 - accuracy: 0.9888 - val_loss: 0.9088 - val_accuracy: 0.8345 - lr: 1.0000e-04\n",
      "Epoch 28/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9869\n",
      "Epoch 28: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 623s 10s/step - loss: 0.0388 - accuracy: 0.9869 - val_loss: 0.8031 - val_accuracy: 0.8367 - lr: 1.0000e-04\n",
      "Epoch 29/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9801\n",
      "Epoch 29: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 622s 10s/step - loss: 0.0612 - accuracy: 0.9801 - val_loss: 1.1246 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
      "Epoch 30/30\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9786\n",
      "Epoch 30: val_loss did not improve from 0.44003\n",
      "65/65 [==============================] - 622s 10s/step - loss: 0.0614 - accuracy: 0.9786 - val_loss: 0.7947 - val_accuracy: 0.8163 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    base_model = DenseNet121(weights = \"imagenet\", include_top = False, input_shape = (224, 224, 3))\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)  # helps with stability\n",
    "    x = Dense(256, activation = \"relu\")(x)\n",
    "    x = Dropout(0.6)(x)  # reduce overfitting\n",
    "    output = Dense(1, activation = \"sigmoid\")(x)  # binary classification\n",
    "\n",
    "    model = Model(inputs = base_model.input, outputs = output)\n",
    "\n",
    "    model.compile(optimizer = Adam(learning_rate = 0.0001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "checkpoint_path = 'model/densenet121_model.h5'\n",
    "callbacks = [\n",
    "    #EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor = \"val_loss\", factor = 0.5, patience = 5, min_lr = 0.0001),\n",
    "    ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor = \"val_loss\",\n",
    "    save_best_only = True,\n",
    "    mode = \"min\",\n",
    "    verbose = 1\n",
    ")\n",
    "]\n",
    "\n",
    "history = model.fit(train_generator, validation_data = valid_generator, epochs = 30, callbacks = callbacks, verbose = 1)\n",
    "\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 25s 2s/step - loss: 0.4806 - accuracy: 0.8027\n",
      "Test Accuracy: 0.8027\n",
      "Test Loss: 0.4806\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", round(test_acc, 4))\n",
    "print(\"Test Loss:\", round(test_loss, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path = 'model_training_plot.png'):\n",
    "    plt.figure(figsize = (12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label = 'Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "    plt.title('Model Accuracy per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label = 'Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "    plt.title('Model Loss per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    print(\"Plot saved successfully at:\", save_path)\n",
    "\n",
    "plot_training_history(history, save_path = 'model_training_plot.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
